@misc{zhuang_survey_2023,
 abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
 author = {Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
 doi = {10.48550/arXiv.2302.01107},
 file = {Preprint PDF:/Users/eric025/Zotero/storage/BSHL5NPJ/Zhuang ç­‰ - 2023 - A Survey on Efficient Training of Transformers.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/628C985Q/2302.html:text/html},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
 language = {en-US},
 month = {May},
 note = {arXiv:2302.01107 [cs]},
 publisher = {arXiv},
 title = {A Survey on Efficient Training of Transformers},
 url = {http://arxiv.org/abs/2302.01107},
 urldate = {2025-03-14},
 year = {2023}
}
