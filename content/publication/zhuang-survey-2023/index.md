---
title: A Survey on Efficient Training of Transformers
authors:
- Bohan Zhuang
- Jing Liu
- Zizheng Pan
- Haoyu He
- Yuetian Weng
- Chunhua Shen
date: '2023-05-01'
publishDate: '2025-03-14T10:03:28.530036Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2302.01107
abstract: Recent advances in Transformers have come with a huge requirement on computing
  resources, highlighting the importance of developing efficient training techniques
  to make Transformer training faster, at lower cost, and to higher accuracy by the
  efficient use of computation and memory resources. This survey provides the first
  systematic overview of the efficient training of Transformers, covering the recent
  progress in acceleration arithmetic and hardware, with a focus on the former. We
  analyze and compare methods that save computation and memory costs for intermediate
  tensors during training, together with techniques on hardware/algorithm co-design.
  We finally discuss challenges and promising areas for future research.
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Computer Vision and Pattern Recognition
- Computer Science - Machine Learning
- ZIPLAB
links:
- name: URL
  url: http://arxiv.org/abs/2302.01107
---
