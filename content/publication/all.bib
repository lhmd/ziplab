
@misc{he_ptqd_2023,
	title = {{PTQD}: {Accurate} {Post}-{Training} {Quantization} for {Diffusion} {Models}},
	shorttitle = {{PTQD}},
	url = {http://arxiv.org/abs/2305.10657},
	doi = {10.48550/arXiv.2305.10657},
	abstract = {Diffusion models have recently dominated image synthesis tasks. However, the iterative denoising process is expensive in computations at inference time, making diffusion models less practical for low-latency and scalable real-world applications. Post-training quantization (PTQ) of diffusion models can significantly reduce the model size and accelerate the sampling process without re-training. Nonetheless, applying existing PTQ methods directly to low-bit diffusion models can significantly impair the quality of generated samples. Specifically, for each denoising step, quantization noise leads to deviations in the estimated mean and mismatches with the predetermined variance schedule. As the sampling process proceeds, the quantization noise may accumulate, resulting in a low signal-to-noise ratio (SNR) during the later denoising steps. To address these challenges, we propose a unified formulation for the quantization noise and diffusion perturbed noise in the quantized denoising process. Specifically, we first disentangle the quantization noise into its correlated and residual uncorrelated parts regarding its full-precision counterpart. The correlated part can be easily corrected by estimating the correlation coefficient. For the uncorrelated part, we subtract the bias from the quantized results to correct the mean deviation and calibrate the denoising variance schedule to absorb the excess variance resulting from quantization. Moreover, we introduce a mixed-precision scheme for selecting the optimal bitwidth for each denoising step. Extensive experiments demonstrate that our method outperforms previous post-training quantized diffusion models, with only a 0.06 increase in FID score compared to full-precision LDM-4 on ImageNet 256x256, while saving 19.9x bit operations. Code is available at https://github.com/ziplab/PTQD.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Liu, Luping and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
	month = nov,
	year = {2023},
	note = {arXiv:2305.10657 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/ZV5AFM7G/He 等 - 2023 - PTQD Accurate Post-Training Quantization for Diffusion Models.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/VLCTLI7E/2305.html:text/html},
}

@misc{zhuang_survey_2023,
	title = {A {Survey} on {Efficient} {Training} of {Transformers}},
	url = {http://arxiv.org/abs/2302.01107},
	doi = {10.48550/arXiv.2302.01107},
	abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
	month = may,
	year = {2023},
	note = {arXiv:2302.01107 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/BSHL5NPJ/Zhuang 等 - 2023 - A Survey on Efficient Training of Transformers.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/628C985Q/2302.html:text/html},
}

@misc{liu_qllm_2024,
	title = {{QLLM}: {Accurate} and {Efficient} {Low}-{Bitwidth} {Quantization} for {Large} {Language} {Models}},
	shorttitle = {{QLLM}},
	url = {http://arxiv.org/abs/2310.08041},
	doi = {10.48550/arXiv.2310.08041},
	abstract = {Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89\% on the average accuracy across five zero-shot tasks.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Gong, Ruihao and Wei, Xiuying and Dong, Zhiwei and Cai, Jianfei and Zhuang, Bohan},
	month = apr,
	year = {2024},
	note = {arXiv:2310.08041 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/MRE4I23B/Liu 等 - 2024 - QLLM Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/AFB73DTW/2310.html:text/html},
}

@misc{he_efficientdm_2024,
	title = {{EfficientDM}: {Efficient} {Quantization}-{Aware} {Fine}-{Tuning} of {Low}-{Bit} {Diffusion} {Models}},
	shorttitle = {{EfficientDM}},
	url = {http://arxiv.org/abs/2310.03270},
	doi = {10.48550/arXiv.2310.03270},
	abstract = {Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion models, where post-training quantization (PTQ) and quantization-aware training (QAT) are two main approaches, each bearing its own properties. While PTQ exhibits efficiency in terms of both time and data usage, it may lead to diminished performance in low bit-width. On the other hand, QAT can alleviate performance degradation but comes with substantial demands on computational and data resources. In this paper, we introduce a data-free and parameter-efficient fine-tuning framework for low-bit diffusion models, dubbed EfficientDM, to achieve QAT-level performance with PTQ-like efficiency. Specifically, we propose a quantization-aware variant of the low-rank adapter (QALoRA) that can be merged with model weights and jointly quantized to low bit-width. The fine-tuning process distills the denoising capabilities of the full-precision model into its quantized counterpart, eliminating the requirement for training data. We also introduce scale-aware optimization and temporal learned step-size quantization to further enhance performance. Extensive experimental results demonstrate that our method significantly outperforms previous PTQ-based diffusion models while maintaining similar time and data efficiency. Specifically, there is only a 0.05 sFID increase when quantizing both weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization speed with comparable generation quality. Code is available at {\textbackslash}href\{https://github.com/ThisisBillhe/EfficientDM\}\{this hrl\}.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
	month = apr,
	year = {2024},
	note = {arXiv:2310.03270 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/6MQWTTF8/He 等 - 2024 - EfficientDM Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/EZFSPWVL/2310.html:text/html},
}

@misc{he_zipcache_2024,
	title = {{ZipCache}: {Accurate} and {Efficient} {KV} {Cache} {Quantization} with {Salient} {Token} {Identification}},
	shorttitle = {{ZipCache}},
	url = {http://arxiv.org/abs/2405.14256},
	doi = {10.48550/arXiv.2405.14256},
	abstract = {KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for LLMs. First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by \$4.98{\textbackslash}times\$, with only a \$0.38{\textbackslash}\%\$ drop in accuracy. In terms of efficiency, ZipCache also showcases a \$37.3{\textbackslash}\%\$ reduction in prefill-phase latency, a \$56.9{\textbackslash}\%\$ reduction in decoding-phase latency, and a \$19.8{\textbackslash}\%\$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of \$4096\$.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan},
	month = may,
	year = {2024},
	note = {arXiv:2405.14256 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/NX8BIA28/He 等 - 2024 - ZipCache Accurate and Efficient KV Cache Quantization with Salient Token Identification.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/8LJVFUL4/2405.html:text/html},
}

@misc{liu_sharpness-aware_2023,
	title = {Sharpness-aware {Quantization} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2111.12273},
	doi = {10.48550/arXiv.2111.12273},
	abstract = {Network quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation terms depend on each other, SAQ can be formulated into three cases, which are analyzed and compared comprehensively. Furthermore, by introducing an efficient training strategy, SAQ only incurs a little additional training overhead compared with the default optimizer (e.g., SGD or AdamW). Extensive experiments on both convolutional neural networks and Transformers across various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102, Oxford-IIIT Pets) show that SAQ improves the generalization performance of the quantized models, yielding the SOTA results in uniform quantization. For example, on ImageNet, SAQ outperforms AdamW by 1.2\% on the Top-1 accuracy for 4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9\% on the Top-1 accuracy.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	month = mar,
	year = {2023},
	note = {arXiv:2111.12273 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/CKRCZXVT/Liu 等 - 2023 - Sharpness-aware Quantization for Deep Neural Networks.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/H242N3WV/2111.html:text/html},
}

@misc{pan_mesa_2022,
	title = {Mesa: {A} {Memory}-saving {Training} {Framework} for {Transformers}},
	shorttitle = {Mesa},
	url = {http://arxiv.org/abs/2111.11124},
	doi = {10.48550/arXiv.2111.11124},
	abstract = {There has been an explosion of interest in designing high-performance Transformers. While Transformers have delivered significant performance improvements, training such networks is extremely memory intensive owing to storing all intermediate activations that are needed for gradient computation during backpropagation, especially for long sequences. To this end, we present Mesa, a memory-saving training framework for Transformers. Specifically, Mesa uses exact activations during forward pass while storing a low-precision version of activations to reduce memory consumption during training. The low-precision activations are then dequantized during back-propagation to compute gradients. Besides, to address the heterogeneous activation distributions in the multi-head self-attention layers, we propose a head-wise activation quantization strategy, which quantizes activations based on the statistics of each head to minimize the approximation error. To further boost training efficiency, we learn quantization parameters by running estimates. More importantly, by re-investing the saved memory in employing a larger batch size or scaling up model size, we may further improve the performance under constrained computational resources. Extensive experiments on ImageNet, CIFAR-100 and ADE20K demonstrate that Mesa can achieve flexible memory-savings (up to 50\%) during training while achieving comparable or even better performance. Code is available at https://github.com/ziplab/Mesa.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Pan, Zizheng and Chen, Peng and He, Haoyu and Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	month = aug,
	year = {2022},
	note = {arXiv:2111.11124 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/XUS2HYMU/Pan 等 - 2022 - Mesa A Memory-saving Training Framework for Transformers.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/E3C7FGM6/2111.html:text/html},
}

@misc{pan_t-stitch_2024,
	title = {T-{Stitch}: {Accelerating} {Sampling} in {Pre}-{Trained} {Diffusion} {Models} with {Trajectory} {Stitching}},
	shorttitle = {T-{Stitch}},
	url = {http://arxiv.org/abs/2402.14167},
	doi = {10.48550/arXiv.2402.14167},
	abstract = {Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL, for example, 40\% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation. We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable diffusion (SD) models but also improve the prompt alignment of stylized SD models from the public model zoo. Code is released at https://github.com/NVlabs/T-Stitch},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Pan, Zizheng and Zhuang, Bohan and Huang, De-An and Nie, Weili and Yu, Zhiding and Xiao, Chaowei and Cai, Jianfei and Anandkumar, Anima},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14167 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/MSMYMFZF/Pan 等 - 2024 - T-Stitch Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/E24AI4I6/2402.html:text/html},
}

@misc{he_zipvl_2024,
	title = {{ZipVL}: {Efficient} {Large} {Vision}-{Language} {Models} with {Dynamic} {Token} {Sparsification}},
	shorttitle = {{ZipVL}},
	url = {http://arxiv.org/abs/2410.08584},
	doi = {10.48550/arXiv.2410.08584},
	abstract = {The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3\${\textbackslash}times\$ and improve decoding throughput by 2.8\${\textbackslash}times\$, with a minimal accuracy reduction of only 0.5{\textbackslash}\% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
	month = dec,
	year = {2024},
	note = {arXiv:2410.08584 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/YFNL7BM9/He 等 - 2024 - ZipVL Efficient Large Vision-Language Models with Dynamic Token Sparsification.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/F8Z56HZR/2410.html:text/html},
}

@misc{yang_object-aware_2024,
	title = {Object-aware {Inversion} and {Reassembly} for {Image} {Editing}},
	url = {http://arxiv.org/abs/2310.12149},
	doi = {10.48550/arXiv.2310.12149},
	abstract = {By comparing the original and target prompts, we can obtain numerous editing pairs, each comprising an object and its corresponding editing target. To allow editability while maintaining fidelity to the input image, existing editing methods typically involve a fixed number of inversion steps that project the whole input image to its noisier latent representation, followed by a denoising process guided by the target prompt. However, we find that the optimal number of inversion steps for achieving ideal editing results varies significantly among different editing pairs, owing to varying editing difficulties. Therefore, the current literature, which relies on a fixed number of inversion steps, produces sub-optimal generation quality, especially when handling multiple editing pairs in a natural image. To this end, we propose a new image editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR), to enable object-level fine-grained editing. Specifically, we design a new search metric, which determines the optimal inversion steps for each editing pair, by jointly considering the editability of the target and the fidelity of the non-editing region. We use our search metric to find the optimal inversion step for each editing pair when editing an image. We then edit these editing pairs separately to avoid concept mismatch. Subsequently, we propose an additional reassembly step to seamlessly integrate the respective editing results and the non-editing region to obtain the final edited image. To systematically evaluate the effectiveness of our method, we collect two datasets called OIRBench for benchmarking single- and multi-object editing, respectively. Experiments demonstrate that our method achieves superior performance in editing object shapes, colors, materials, categories, etc., especially in multi-object editing scenarios.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Yang, Zhen and Ding, Ganggui and Wang, Wen and Chen, Hao and Zhuang, Bohan and Shen, Chunhua},
	month = mar,
	year = {2024},
	note = {arXiv:2310.12149 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/2WSKZE52/Yang 等 - 2024 - Object-aware Inversion and Reassembly for Image Editing.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/BNVPTYCH/2310.html:text/html},
}

@misc{chen_mvsplat360_2024,
	title = {{MVSplat360}: {Feed}-{Forward} 360 {Scene} {Synthesis} from {Sparse} {Views}},
	shorttitle = {{MVSplat360}},
	url = {http://arxiv.org/abs/2411.04924},
	doi = {10.48550/arXiv.2411.04924},
	abstract = {We introduce MVSplat360, a feed-forward approach for 360\{{\textbackslash}deg\} novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360's performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360\{{\textbackslash}deg\} NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. The video results are available on our project page: https://donydchen.github.io/mvsplat360.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Chen, Yuedong and Zheng, Chuanxia and Xu, Haofei and Zhuang, Bohan and Vedaldi, Andrea and Cham, Tat-Jen and Cai, Jianfei},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04924 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/VWLLIFCW/Chen 等 - 2024 - MVSplat360 Feed-Forward 360 Scene Synthesis from Sparse Views.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/MZS23MME/2411.html:text/html},
}

@misc{liu_focusformer_2022,
	title = {{FocusFormer}: {Focusing} on {What} {We} {Need} via {Architecture} {Sampler}},
	shorttitle = {{FocusFormer}},
	url = {http://arxiv.org/abs/2208.10861},
	doi = {10.48550/arXiv.2208.10861},
	abstract = {Vision Transformers (ViTs) have underpinned the recent breakthroughs in computer vision. However, designing the architectures of ViTs is laborious and heavily relies on expert knowledge. To automate the design process and incorporate deployment flexibility, one-shot neural architecture search decouples the supernet training and architecture specialization for diverse deployment scenarios. To cope with an enormous number of sub-networks in the supernet, existing methods treat all architectures equally important and randomly sample some of them in each update step during training. During architecture search, these methods focus on finding architectures on the Pareto frontier of performance and resource consumption, which forms a gap between training and deployment. In this paper, we devise a simple yet effective method, called FocusFormer, to bridge such a gap. To this end, we propose to learn an architecture sampler to assign higher sampling probabilities to those architectures on the Pareto frontier under different resource constraints during supernet training, making them sufficiently optimized and hence improving their performance. During specialization, we can directly use the well-trained architecture sampler to obtain accurate architectures satisfying the given resource constraint, which significantly improves the search efficiency. Extensive experiments on CIFAR-100 and ImageNet show that our FocusFormer is able to improve the performance of the searched architectures while significantly reducing the search cost. For example, on ImageNet, our FocusFormer-Ti with 1.4G FLOPs outperforms AutoFormer-Ti by 0.5\% in terms of the Top-1 accuracy.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	month = aug,
	year = {2022},
	note = {arXiv:2208.10861 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/7DPRBTKA/Liu 等 - 2022 - FocusFormer Focusing on What We Need via Architecture Sampler.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/TPT48692/2208.html:text/html},
}

@misc{wang_switchgpt_2023,
	title = {{SwitchGPT}: {Adapting} {Large} {Language} {Models} for {Non}-{Text} {Outputs}},
	shorttitle = {{SwitchGPT}},
	url = {http://arxiv.org/abs/2309.07623},
	doi = {10.48550/arXiv.2309.07623},
	abstract = {Large Language Models (LLMs), primarily trained on text-based datasets, exhibit exceptional proficiencies in understanding and executing complex linguistic instructions via text outputs. However, they falter when requests to generate non-text ones. Concurrently, modality conversion models, such as text-to-image, despite generating high-quality images, suffer from a lack of extensive textual pretraining. As a result, these models are only capable of accommodating specific image descriptions rather than comprehending more complex instructions. To bridge this gap, we propose a novel approach, {\textbackslash}methodname, from a modality conversion perspective that evolves a text-based LLM into a multi-modal one. We specifically employ a minimal dataset to instruct LLMs to recognize the intended output modality as directed by the instructions. Consequently, the adapted LLM can effectively summon various off-the-shelf modality conversion models from the model zoos to generate non-text responses. This circumvents the necessity for complicated pretraining that typically requires immense quantities of paired multi-modal data, while simultaneously inheriting the extensive knowledge of LLMs and the ability of high-quality generative models. To evaluate and compare the adapted multi-modal LLM with its traditional counterparts, we have constructed a multi-modal instruction benchmark that solicits diverse modality outputs. The experiment results reveal that, with minimal training, LLMs can be conveniently adapted to comprehend requests for non-text responses, thus achieving higher flexibility in multi-modal scenarios. Code and data will be made available at https://github.com/xinke-wang/SwitchGPT.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07623 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/ZDFZCV43/Wang 等 - 2023 - SwitchGPT Adapting Large Language Models for Non-Text Outputs.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/GJR3DVLD/2309.html:text/html},
}

@misc{liu_me-switch_2024,
	title = {{ME}-{Switch}: {A} {Memory}-{Efficient} {Expert} {Switching} {Framework} for {Large} {Language} {Models}},
	shorttitle = {{ME}-{Switch}},
	url = {http://arxiv.org/abs/2406.09041},
	doi = {10.48550/arXiv.2406.09041},
	abstract = {LLM development involves pre-training a foundation model on massive data, followed by fine-tuning on task-specific data to create specialized experts. Serving these experts can pose significant memory challenges, as loading all experts onto devices is impractical, and frequent switching between experts in response to user requests can incur substantial I/O costs. Previous approaches decompose the expert weights as the pre-trained weights plus delta weights, followed by quantizing the delta weights using output channel-wise step sizes to reduce the model size. However, these methods overlook the fact that certain input channels of delta weights can cause significant quantization errors at extremely low bitwidths. Additionally, existing methods assume that the appropriate model for a user request is known in advance, which is not the case in practice. To this end, we introduce ME-Switch, a memory-efficient expert switching framework tailored for serving multiple LLMs. To condense the number of bits required for describing the delta weights, we propose a salient-aware delta compression method that identifies salient input channels based on reconstruction error and applies mixed-precision quantization, reducing non-salient channels to low bits while keeping salient ones intact, cutting storage demand without compromising performance. Moreover, we develop a model-level routing method that efficiently directs user queries to the most suitable expert by performing domain classification. Extensive experiments show the promising memory efficiency and routing performance of ME-Switch. For example, when serving three models from the Mistral-7B family, ME-Switch reduces the model size by \$1.74{\textbackslash}times\$ and maintains nearly lossless performance on instruction, mathematical reasoning, and code generation tasks. Notably, our method can efficiently serve 16 Mistral-7B models on a single NVIDIA A100 GPU.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Gong, Ruihao and Zhang, Mingyang and He, Yefei and Cai, Jianfei and Zhuang, Bohan},
	month = oct,
	year = {2024},
	note = {arXiv:2406.09041 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/URWXTEYN/Liu 等 - 2024 - ME-Switch A Memory-Efficient Expert Switching Framework for Large Language Models.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/TQY7BUDB/2406.html:text/html},
}

@misc{he_zipar_2024,
	title = {{ZipAR}: {Accelerating} {Auto}-regressive {Image} {Generation} through {Spatial} {Locality}},
	shorttitle = {{ZipAR}},
	url = {http://arxiv.org/abs/2412.04062},
	doi = {10.48550/arXiv.2412.04062},
	abstract = {In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91\% on the Emu3-Gen model without requiring any additional retraining. Code is available here: https://github.com/ThisisBillhe/ZipAR.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Chen, Feng and He, Yuanyu and He, Shaoxuan and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.04062 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/EQY2RPZW/He 等 - 2024 - ZipAR Accelerating Auto-regressive Image Generation through Spatial Locality.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/GQFMDVMY/2412.html:text/html},
}

@misc{chen_enhancing_2024,
	title = {Enhancing {Perception} {Capabilities} of {Multimodal} {LLMs} with {Training}-{Free} {Fusion}},
	url = {http://arxiv.org/abs/2412.01289},
	doi = {10.48550/arXiv.2412.01289},
	abstract = {Multimodal LLMs (MLLMs) equip language models with visual capabilities by aligning vision encoders with language models. Existing methods to enhance the visual perception of MLLMs often involve designing more powerful vision encoders, which requires exploring a vast design space and re-aligning each potential encoder with the language model, resulting in prohibitively high training costs. In this paper, we introduce VisionFuse, a novel integration framework that efficiently utilizes multiple vision encoders from off-the-shelf MLLMs to enhance visual perception without requiring additional training. Our approach is motivated by the observation that different MLLMs tend to focus on distinct regions given the same query and image. Moreover, we find that the feature distributions of vision encoders within an MLLM family, a group of MLLMs sharing the same pretrained LLM, are highly aligned. Building on these insights, VisionFuse enriches the visual context by concatenating the tokens generated by the vision encoders of selected MLLMs within a family. By merging the parameters of language models from these MLLMs, VisionFuse allows a single language model to align with various vision encoders, significantly reducing deployment overhead. We conduct comprehensive evaluations across multiple multimodal benchmarks using various MLLM combinations, demonstrating substantial improvements in multimodal tasks. Notably, when integrating MiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase of over 4\%.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Chen, Zhuokun and Hu, Jinwu and Deng, Zeshuai and Wang, Yufeng and Zhuang, Bohan and Tan, Mingkui},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01289 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/WNJPI5SP/Chen 等 - 2024 - Enhancing Perception Capabilities of Multimodal LLMs with Training-Free Fusion.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/IY3KVUBP/2412.html:text/html},
}

@misc{chen_evaluating_2024,
	title = {Evaluating and {Advancing} {Multimodal} {Large} {Language} {Models} in {Ability} {Lens}},
	url = {http://arxiv.org/abs/2411.14725},
	doi = {10.48550/arXiv.2411.14725},
	abstract = {As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of {\textbackslash}textbf\{vision perception\} abilities, the foundational skill of MLLMs. We find that existing perception benchmarks, each focusing on different question types, domains, and evaluation metrics, introduce significant evaluation variance, complicating comprehensive assessments of perception abilities when relying on any single benchmark. To address this, we introduce {\textbackslash}textbf\{AbilityLens\}, a unified benchmark designed to evaluate MLLMs across six key perception abilities, focusing on both accuracy and stability, with each ability encompassing diverse question types, domains, and metrics. With the assistance of AbilityLens, we: (1) identify the strengths and weaknesses of current models, highlighting stability patterns and revealing a notable performance gap between open-source and closed-source models; (2) introduce an online evaluation mode, which uncovers interesting ability conflict and early convergence phenomena during MLLM training; and (3) design a simple ability-specific model merging method that combines the best ability checkpoint from early training stages, effectively mitigating performance decline due to ability conflict. The benchmark and online leaderboard will be released soon.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Chen, Feng and Gou, Chenhui and Liu, Jing and Yang, Yang and Li, Zhaoyang and Zhang, Jiyuan and Sun, Zhenbang and Zhuang, Bohan and Wu, Qi},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14725 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/S9ICSQVN/Chen 等 - 2024 - Evaluating and Advancing Multimodal Large Language Models in Ability Lens.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/PMYGYS73/2411.html:text/html},
}

@misc{wang_are_2025,
	title = {Are {Large} {Vision} {Language} {Models} {Good} {Game} {Players}?},
	url = {http://arxiv.org/abs/2503.02358},
	doi = {10.48550/arXiv.2503.02358},
	abstract = {Large Vision Language Models (LVLMs) have demonstrated remarkable abilities in understanding and reasoning about both visual and textual information. However, existing evaluation methods for LVLMs, primarily based on benchmarks like Visual Question Answering and image captioning, often fail to capture the full scope of LVLMs' capabilities. These benchmarks are limited by issues such as inadequate assessment of detailed visual perception, data contamination, and a lack of focus on multi-turn reasoning. To address these challenges, we propose {\textbackslash}method\{\}, a game-based evaluation framework designed to provide a comprehensive assessment of LVLMs' cognitive and reasoning skills in structured environments. {\textbackslash}method\{\} uses a set of games to evaluate LVLMs on four core tasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing, with each target task designed to assess specific abilities, including visual perception, reasoning, decision-making, etc. Based on this framework, we conduct extensive experiments that explore the limitations of current LVLMs, such as handling long structured outputs and perceiving detailed and dense elements. Code and data are publicly available at https://github.com/xinke-wang/LVLM-Playground.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
	month = mar,
	year = {2025},
	note = {arXiv:2503.02358 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/6FK3649S/Wang 等 - 2025 - Are Large Vision Language Models Good Game Players.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/MXWY4K3V/2503.html:text/html},
}

@misc{zhang_channel_2024,
	title = {Channel {Merging}: {Preserving} {Specialization} for {Merged} {Experts}},
	shorttitle = {Channel {Merging}},
	url = {http://arxiv.org/abs/2412.15283},
	doi = {10.48550/arXiv.2412.15283},
	abstract = {Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53\% parameters when used with a task-specific router.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Zhang, Mingyang and Liu, Jing and Ding, Ganggui and Yu, Xinyi and Ou, Linlin and Zhuang, Bohan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15283 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, ZIPLAB},
	file = {Preprint PDF:/Users/eric025/Zotero/storage/F8X7LIEP/Zhang 等 - 2024 - Channel Merging Preserving Specialization for Merged Experts.pdf:application/pdf;Snapshot:/Users/eric025/Zotero/storage/YFF6FT3Q/2412.html:text/html},
}

@article{pan_fast_2022,
	title = {Fast vision transformers with hilo attention},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d5f703ee1dedbfe324b1872f44db939-Abstract-Conference.html},
	language = {en-US},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
	year = {2022},
	keywords = {/unread, ZIPLAB},
	pages = {14541--14554},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/HDABLB5I/Pan 等 - 2022 - Fast vision transformers with hilo attention.pdf:application/pdf},
}

@article{liu_discrimination-aware_2021,
	title = {Discrimination-aware network pruning for deep model compression},
	volume = {44},
	url = {https://ieeexplore.ieee.org/abstract/document/9384353/},
	language = {en-US},
	number = {8},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Jing and Zhuang, Bohan and Zhuang, Zhuangwei and Guo, Yong and Huang, Junzhou and Zhu, Jinhui and Tan, Mingkui},
	year = {2021},
	note = {Publisher: IEEE},
	keywords = {ZIPLAB},
	pages = {4035--4051},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/ANMUMQMN/Liu 等 - 2021 - Discrimination-aware network pruning for deep model compression.pdf:application/pdf},
}

@article{he_pruning_2024,
	title = {Pruning self-attentions into convolutional layers in single path},
	volume = {46},
	url = {https://ieeexplore.ieee.org/abstract/document/10409620/},
	number = {5},
	urldate = {2025-03-14},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {He, Haoyu and Cai, Jianfei and Liu, Jing and Pan, Zizheng and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
	year = {2024},
	note = {Publisher: IEEE},
	keywords = {/unread, ZIPLAB},
	pages = {3910--3922},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/Y2M78MME/He 等 - 2024 - Pruning self-attentions into convolutional layers in single path.pdf:application/pdf},
}

@article{liu_ecoformer_2022,
	title = {Ecoformer: {Energy}-saving attention with linear complexity},
	volume = {35},
	shorttitle = {Ecoformer},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/4310ae054ce265e56d8ea897971149b5-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Jing and Pan, Zizheng and He, Haoyu and Cai, Jianfei and Zhuang, Bohan},
	year = {2022},
	keywords = {/unread, ZIPLAB},
	pages = {10295--10308},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/XYJJRXJL/Liu 等 - 2022 - Ecoformer Energy-saving attention with linear complexity.pdf:application/pdf},
}

@article{liu_minicache_2024,
	title = {Minicache: {Kv} cache compression in depth dimension for large language models},
	volume = {37},
	shorttitle = {Minicache},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/fd0705710bf01b88a60a3d479ea341d9-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Reza and Zhuang, Bohan},
	year = {2024},
	keywords = {/unread, ZIPLAB},
	pages = {139997--140031},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/V4ZWTC6B/Liu 等 - 2024 - Minicache Kv cache compression in depth dimension for large language models.pdf:application/pdf},
}

@article{deng_efficient_2023,
	title = {Efficient test-time adaptation for super-resolution with second-order degradation and reconstruction},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ec3d49763c653ad7c8d587f52220c129-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Deng, Zeshuai and Chen, Zhuokun and Niu, Shuaicheng and Li, Thomas and Zhuang, Bohan and Tan, Mingkui},
	year = {2023},
	keywords = {/unread, ZIPLAB},
	pages = {74671--74701},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/GWP7YILF/Deng 等 - 2023 - Efficient test-time adaptation for super-resolution with second-order degradation and reconstruction.pdf:application/pdf},
}

@article{weng_mask_2023,
	title = {Mask propagation for efficient video semantic segmentation},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/167bcf2af2cd08fcf75b932022db0311-Abstract-Conference.html},
	language = {en-US},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Weng, Yuetian and Han, Mingfei and He, Haoyu and Li, Mingjie and Yao, Lina and Chang, Xiaojun and Zhuang, Bohan},
	year = {2023},
	keywords = {ZIPLAB},
	pages = {7170--7183},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/QCXSFFDN/Weng 等 - 2023 - Mask propagation for efficient video semantic segmentation.pdf:application/pdf},
}

@article{liu_single-path_2023,
	title = {Single-path bit sharing for automatic loss-aware model compression},
	volume = {45},
	url = {https://ieeexplore.ieee.org/abstract/document/10122994/},
	number = {10},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Jing and Zhuang, Bohan and Chen, Peng and Shen, Chunhua and Cai, Jianfei and Tan, Mingkui},
	year = {2023},
	note = {Publisher: IEEE},
	keywords = {ZIPLAB},
	pages = {12459--12473},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/CSZ29RPR/Liu 等 - 2023 - Single-path bit sharing for automatic loss-aware model compression.pdf:application/pdf},
}

@article{he_end--end_2023,
	title = {End-to-end one-shot human parsing},
	volume = {45},
	url = {https://ieeexplore.ieee.org/abstract/document/10207820/},
	number = {12},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Haoyu and Zhang, Jing and Zhuang, Bohan and Cai, Jianfei and Tao, Dacheng},
	year = {2023},
	note = {Publisher: IEEE},
	keywords = {/unread, ZIPLAB},
	pages = {14481--14496},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/7ADB3RQR/He 等 - 2023 - End-to-end one-shot human parsing.pdf:application/pdf},
}

@article{chen_gmai-mmbench_2024,
	title = {{GMAI}-{MMBench}: {A} {Comprehensive} {Multimodal} {Evaluation} {Benchmark} {Towards} {General} {Medical} {AI}},
	volume = {37},
	shorttitle = {{GMAI}-{MMBench}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/ab7e02fd60e47e2a379d567f6b54f04e-Abstract-Datasets_and_Benchmarks_Track.html},
	language = {en},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and Wang, Benyou and Zhang, Shaoting and Fu, Bin and Cai, Jianfei and Zhuang, Bohan and Seibel, Eric J. and Qiao, Yu and He, Junjun},
	month = dec,
	year = {2024},
	keywords = {/unread, ZIPLAB},
	pages = {94327--94427},
	file = {Full Text PDF:/Users/eric025/Zotero/storage/BXDJIE4M/Chen 等 - 2024 - GMAI-MMBench A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI.pdf:application/pdf},
}

@inproceedings{pan_scalable_2021,
	title = {Scalable vision transformers with hierarchical pooling},
	url = {http://openaccess.thecvf.com/content/ICCV2021/html/Pan_Scalable_Vision_Transformers_With_Hierarchical_Pooling_ICCV_2021_paper.html},
	language = {en-US},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/cvf international conference on computer vision},
	author = {Pan, Zizheng and Zhuang, Bohan and Liu, Jing and He, Haoyu and Cai, Jianfei},
	year = {2021},
	keywords = {ZIPLAB},
	pages = {377--386},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/VXH9TACE/Pan 等 - 2021 - Scalable vision transformers with hierarchical pooling.pdf:application/pdf},
}

@incollection{leonardis_mvsplat_2025,
	address = {Cham},
	title = {{MVSplat}: {Efficient} {3D} {Gaussian} {Splatting} from {Sparse} {Multi}-view {Images}},
	volume = {15079},
	isbn = {978-3-031-72663-7 978-3-031-72664-4},
	shorttitle = {{MVSplat}},
	url = {https://link.springer.com/10.1007/978-3-031-72664-4_21},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-72664-4_21},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ZIPLAB},
	pages = {370--386},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/S64M2HRT/Chen 等 - 2025 - MVSplat Efficient 3D Gaussian Splatting from Sparse Multi-view Images.pdf:application/pdf},
}

@inproceedings{pan_less_2022,
	title = {Less is more: {Pay} less attention in vision transformers},
	volume = {36},
	shorttitle = {Less is more},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20099},
	language = {en-US},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Pan, Zizheng and Zhuang, Bohan and He, Haoyu and Liu, Jing and Cai, Jianfei},
	year = {2022},
	note = {Issue: 2},
	keywords = {ZIPLAB},
	pages = {2035--2043},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/PJ525ELR/Pan 等 - 2022 - Less is more Pay less attention in vision transformers.pdf:application/pdf},
}

@inproceedings{zhang_motion_2024,
	title = {Motion mamba: {Efficient} and long sequence motion generation},
	shorttitle = {Motion mamba},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Zhang, Zeyu and Liu, Akide and Reid, Ian and Hartley, Richard and Zhuang, Bohan and Tang, Hao},
	year = {2024},
	keywords = {/unread, ZIPLAB},
	pages = {265--282},
}

@inproceedings{he_sensitivity-aware_2023,
	title = {Sensitivity-aware visual parameter-efficient fine-tuning},
	url = {http://openaccess.thecvf.com/content/ICCV2023/html/He_Sensitivity-Aware_Visual_Parameter-Efficient_Fine-Tuning_ICCV_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {He, Haoyu and Cai, Jianfei and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
	year = {2023},
	keywords = {/unread, ZIPLAB},
	pages = {11825--11835},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/GLTDCVPF/He 等 - 2023 - Sensitivity-aware visual parameter-efficient fine-tuning.pdf:application/pdf},
}

@inproceedings{li_automated_2022,
	title = {Automated progressive learning for efficient training of vision transformers},
	url = {http://openaccess.thecvf.com/content/CVPR2022/html/Li_Automated_Progressive_Learning_for_Efficient_Training_of_Vision_Transformers_CVPR_2022_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Changlin and Zhuang, Bohan and Wang, Guangrun and Liang, Xiaodan and Chang, Xiaojun and Yang, Yi},
	year = {2022},
	keywords = {/unread, ZIPLAB},
	pages = {12486--12496},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/6NFIZN3Q/Li 等 - 2022 - Automated progressive learning for efficient training of vision transformers.pdf:application/pdf},
}

@inproceedings{he_bivit_2023,
	title = {Bivit: {Extremely} compressed binary vision transformers},
	shorttitle = {Bivit},
	url = {http://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {He, Yefei and Lou, Zhenyu and Zhang, Luoming and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
	year = {2023},
	keywords = {/unread, ZIPLAB},
	pages = {5651--5663},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/HNRCVW9B/He 等 - 2023 - Bivit Extremely compressed binary vision transformers.pdf:application/pdf},
}

@inproceedings{chen_aqd_2021,
	title = {Aqd: {Towards} accurate quantized object detection},
	shorttitle = {Aqd},
	url = {http://openaccess.thecvf.com/content/CVPR2021/html/Chen_AQD_Towards_Accurate_Quantized_Object_Detection_CVPR_2021_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Chen, Peng and Liu, Jing and Zhuang, Bohan and Tan, Mingkui and Shen, Chunhua},
	year = {2021},
	keywords = {/unread, ZIPLAB},
	pages = {104--113},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/7LUD42VI/Chen 等 - 2021 - Aqd Towards accurate quantized object detection.pdf:application/pdf},
}

@incollection{avidan_efficient_2022,
	address = {Cham},
	title = {An {Efficient} {Spatio}-{Temporal} {Pyramid} {Transformer} for {Action} {Detection}},
	volume = {13694},
	isbn = {978-3-031-19829-8 978-3-031-19830-4},
	url = {https://link.springer.com/10.1007/978-3-031-19830-4_21},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Weng, Yuetian and Pan, Zizheng and Han, Mingfei and Chang, Xiaojun and Zhuang, Bohan},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19830-4_21},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ZIPLAB},
	pages = {358--375},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/AKYFRX6Z/Weng 等 - 2022 - An Efficient Spatio-Temporal Pyramid Transformer for Action Detection.pdf:application/pdf},
}

@incollection{leonardis_longvlm_2025,
	address = {Cham},
	title = {{LongVLM}: {Efficient} {Long} {Video} {Understanding} via {Large} {Language} {Models}},
	volume = {15091},
	isbn = {978-3-031-73413-7 978-3-031-73414-4},
	shorttitle = {{LongVLM}},
	url = {https://link.springer.com/10.1007/978-3-031-73414-4_26},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-73414-4_26},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ZIPLAB},
	pages = {453--470},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/MIGH5A5C/Weng 等 - 2025 - LongVLM Efficient Long Video Understanding via Large Language Models.pdf:application/pdf},
}

@inproceedings{pan_stitchable_2023,
	title = {Stitchable neural networks},
	url = {http://openaccess.thecvf.com/content/CVPR2023/html/Pan_Stitchable_Neural_Networks_CVPR_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
	year = {2023},
	keywords = {ZIPLAB},
	pages = {16102--16112},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/H7SDREWZ/Pan 等 - 2023 - Stitchable neural networks.pdf:application/pdf},
}

@inproceedings{liu_sa-bnn_2021,
	title = {{SA}-{BNN}: {State}-aware binary neural network},
	volume = {35},
	shorttitle = {{SA}-{BNN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16306},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Chunlei and Chen, Peng and Zhuang, Bohan and Shen, Chunhua and Zhang, Baochang and Ding, Wenrui},
	year = {2021},
	note = {Issue: 3},
	keywords = {/unread, ZIPLAB},
	pages = {2091--2099},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/N3NLD9KH/Liu 等 - 2021 - SA-BNN State-aware binary neural network.pdf:application/pdf},
}

@inproceedings{wang_modaverse_2024,
	title = {Modaverse: {Efficiently} transforming modalities with llms},
	shorttitle = {Modaverse},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/Wang_ModaVerse_Efficiently_Transforming_Modalities_with_LLMs_CVPR_2024_paper.html},
	language = {en-US},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
	year = {2024},
	keywords = {ZIPLAB},
	pages = {26606--26616},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/6ZTIUWAW/Wang 等 - 2024 - Modaverse Efficiently transforming modalities with llms.pdf:application/pdf},
}

@inproceedings{he_dynamic_2023,
	title = {Dynamic focus-aware positional queries for semantic segmentation},
	url = {http://openaccess.thecvf.com/content/CVPR2023/html/He_Dynamic_Focus-Aware_Positional_Queries_for_Semantic_Segmentation_CVPR_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {He, Haoyu and Cai, Jianfei and Pan, Zizheng and Liu, Jing and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
	year = {2023},
	keywords = {/unread, ZIPLAB},
	pages = {11299--11308},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/TU9VGRTK/He 等 - 2023 - Dynamic focus-aware positional queries for semantic segmentation.pdf:application/pdf},
}

@inproceedings{chen_fatnn_2021,
	title = {{FATNN}: {Fast} and accurate ternary neural networks},
	shorttitle = {{FATNN}},
	url = {http://openaccess.thecvf.com/content/ICCV2021/html/Chen_FATNN_Fast_and_Accurate_Ternary_Neural_Networks_ICCV_2021_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Chen, Peng and Zhuang, Bohan and Shen, Chunhua},
	year = {2021},
	keywords = {/unread, ZIPLAB},
	pages = {5219--5228},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/UHF5X4ER/Chen 等 - 2021 - FATNN Fast and accurate ternary neural networks.pdf:application/pdf},
}

@incollection{linguraru_sam-med3d-moe_2024,
	address = {Cham},
	title = {{SAM}-{Med3D}-{MoE}: {Towards} a {Non}-{Forgetting} {Segment} {Anything} {Model} via {Mixture} of {Experts} for {3D} {Medical} {Image} {Segmentation}},
	volume = {15009},
	isbn = {978-3-031-72113-7 978-3-031-72114-4},
	shorttitle = {{SAM}-{Med3D}-{MoE}},
	url = {https://link.springer.com/10.1007/978-3-031-72114-4_53},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Wang, Guoan and Ye, Jin and Cheng, Junlong and Li, Tianbin and Chen, Zhaolin and Cai, Jianfei and He, Junjun and Zhuang, Bohan},
	editor = {Linguraru, Marius George and Dou, Qi and Feragen, Aasa and Giannarou, Stamatia and Glocker, Ben and Lekadir, Karim and Schnabel, Julia A.},
	year = {2024},
	doi = {10.1007/978-3-031-72114-4_53},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ZIPLAB},
	pages = {552--561},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/U8DW3VCK/Wang 等 - 2024 - SAM-Med3D-MoE Towards a Non-Forgetting Segment Anything Model via Mixture of Experts for 3D Medical.pdf:application/pdf},
}

@inproceedings{he_efficient_2024,
	title = {Efficient stitchable task adaptation},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/He_Efficient_Stitchable_Task_Adaptation_CVPR_2024_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Haoyu and Pan, Zizheng and Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	year = {2024},
	keywords = {/unread, ZIPLAB},
	pages = {28555--28565},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/CABSHEJI/He 等 - 2024 - Efficient stitchable task adaptation.pdf:application/pdf},
}

@incollection{leonardis_stitched_2025,
	address = {Cham},
	title = {Stitched {ViTs} are {Flexible} {Vision} {Backbones}},
	volume = {15099},
	isbn = {978-3-031-72939-3 978-3-031-72940-9},
	url = {https://link.springer.com/10.1007/978-3-031-72940-9_15},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Pan, Zizheng and Liu, Jing and He, Haoyu and Cai, Jianfei and Zhuang, Bohan},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-72940-9_15},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {/unread, ZIPLAB},
	pages = {258--274},
	file = {Available Version (via Google Scholar):/Users/eric025/Zotero/storage/N2KXCWNC/Pan 等 - 2025 - Stitched ViTs are Flexible Vision Backbones.pdf:application/pdf},
}
@article{zhuang2021effective,
  title={Effective training of convolutional neural networks with low-bitwidth weights and activations},
  author={Zhuang, Bohan and Tan, Mingkui and Liu, Jing and Liu, Lingqiao and Reid, Ian and Shen, Chunhua},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={10},
  pages={6140--6152},
  year={2021},
  publisher={IEEE}
}
@inproceedings{zhuang2019structured,
  title={Structured binary neural networks for accurate image classification and semantic segmentation},
  author={Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={413--422},
  year={2019}
}